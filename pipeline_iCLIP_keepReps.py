"""===========================
Pipeline template
===========================

.. Replace the documentation below with your own description of the
   pipeline's purpose

Overview
========

This pipeline computes the word frequencies in the configuration
files :file:``pipeline.ini` and :file:`conf.py`.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_@template@.py config

Input files
-----------

None required except the pipeline configuration files.

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *
import sys
import os
import CGAT.Experiment as E
import CGATPipelines.Pipeline as P

# load options from the config file
PARAMS = P.getParameters(
    ["%s/pipeline.ini" % os.path.splitext(__file__)[0],
     "../pipeline.ini",
     "pipeline.ini"])


# ---------------------------------------------------
# Specific pipeline tasks
    
''' below, use this to change environment before calling tool/script
need to change env name from py36-v1 to desired one'''
# PATH=/t1-data/user/nsampaio/py36-v1/conda-install/envs/py36-v1/bin;
# CONDA_PREFIX=/t1-data/user/nsampaio/py36-v1/conda-install/envs/py36-v1;

''' below use command to estimate command computing time and resources'''
#   /usr/bin/time -o %(startfile)s.time -v


## Cutadapt
#@follows(mkdir("processed"))
#@transform('*.fastq.gz', regex(r'(.*).fastq.gz'), r'processed/\1.trim.fastq.gz') 
#def cutadapt(infile, outfile):
#    ''' trims 3' adapter and removes low quality reads '''
#    statement = ''' cutadapt -q %(cutadapt_minphred)s 
#    --minimum-length %(cutadapt_minlength)s 
#    -a %(general_adapter)s
#    -o %(outfile)s %(infile)s
#    '''
#    P.run()
#  
#    
## Remove UMI
#@transform(cutadapt, suffix('.trim.fastq.gz'), '.processed.fastq.gz')
#def umiprocess(infile, outfile):
#    ''' removes UMI from start of read and moves to end of read name '''
#    statement = ''' umi_tools extract -I %(infile)s 
#    --bc-pattern=%(umiprocess_pattern)s
#    --stdout=%(outfile)s
#    '''
#    P.run()
#
#    
## Demultiplexing ### need to figure out how to redirect to new folder
#@follows(mkdir("demultiplexed"))
#@split(umiprocess, 'demultiplexed/*.fastq')
#def demux(infile, outfiles):    
#    ''' demultiplex based on barcode at start of read using fastx-toolkit '''
#    startfile = " ".join(infile)
#    statement = '''  zcat %(startfile)s |
#    fastx_barcode_splitter.pl --bcfile %(demux_barcodes)s 
#    --prefix demux --suffix .fastq --bol 
#    --mismatches %(demux_mismatches)i 
#    --partial %(demux_partial)i 
#    '''
#    P.run()
#
#
## FASTQC
#@follows(mkdir("fastqc1"))
#@transform('demux*.fastq', regex(r'demux(.*).fastq'), 'fastqc1/.fastqc')
#def fastqc1(infile,outfile):
#    ''' does fastqc on reads after processing '''
#    statement = ''' fastqc %(infile)s -o %(general_outputdir)s/fastqc1 > %(outfile)s
#    '''
#    P.run()
#    
#    
## STAR remove Reps
##@follows(fastqc1)
##@follows(mkdir("mappedreps"), demux)
##@transform('demux*.fastq', regex(r'demux(.*).fastq'), r'mappedreps/\1.rep.bam')
##def STARrmRep(infile, outfile):
##    ''' maps to repetitive elements, produces 2 files:
##        mapped - name.bam; unmapped - nameUnmapped.mate1 '''
##    outprefix = P.snip(outfile, ".bam")
##    job_threads = PARAMS["STARrmRep_threads"]
##    statement = '''
##    STAR  --runMode alignReads
##    --runThreadN %(job_threads)i
##    --genomeDir %(STARrmRep_repbase)s
##    --limitBAMsortRAM 10000000000
##    --readFilesIn %(infile)s
##    --outSAMunmapped Within
##    --outFilterMultimapNmax 30
##    --outFilterMultimapScoreRange 1
##    --outFileNamePrefix %(outprefix)s
##    --outSAMattributes All
##    --outStd BAM_SortedByCoordinate
##    --outSAMtype BAM SortedByCoordinate
##    --outFilterType BySJout
##    --outReadsUnmapped Fastx
##    --outFilterScoreMin 10
##    --alignEndsType Extend5pOfRead1
##    > %(outfile)s
##    '''
##    P.run()
##   
##    
### samtools index1
##@transform(STARrmRep, suffix('.rep.bam'), '.rep.bam.bai')
##def rep_index(infile, outfile):
##    statement = ''' samtools index %(infile)s
##    '''
##    P.run()
#
#
### Deduplicate
##@follows(rep_index)
##@transform(STARrmRep, regex(r'(.*).rep.bam'), r'\1.rep.dedup.bam')
##def rep_dedup(infile,outfile):
##    ''' deduplicate samples based on UMI using umi_tools '''
##    statement = '''
##    umi_tools dedup -I %(infile)s --output-stats=deduplicated -S %(outfile)s
##    '''
##    P.run()
##
##
### Count Reps
##@transform(rep_dedup, suffix('.rep.dedup.bam'), '.rep.metrics')
##def countRep(infile, outfile):
##    '''counts number reads mapping to each repetitive element'''
##    statement = '''
##    PATH=/t1-data/user/nsampaio/py36-v1/conda-install/envs/Py2/bin;
##    CONDA_PREFIX=/t1-data/user/nsampaio/py36-v1/conda-install/envs/Py2;
##    samtools view %(infile)s | 
##    /t1-data/user/nsampaio/software/gscripts/gscripts/general/count_aligned_from_sam.py 
##    > %(outfile)s
##    '''
##    P.run()
###    
##    
### FASTQC
##@follows(STARrmRep)
##@follows(mkdir("fastqc2"))
##@transform('mappedreps/*repUnmapped.out.mate1', regex(r'mappedreps/(.*).repUnmapped.out.mate1'), r'fastqc2/\1.fastqc')
##def fastqc2(infile,outfile):
##    ''' does fastqc on mapped repetitive elements from STARrmRep '''
##    statement = ''' fastqc %(infile)s -o %(general_outputdir)s/fastqc2 > %(outfile)s
##    '''
##    P.run()


# STAR mapping 
#@follows(STARrmRep)
@follows(mkdir("STARmapped"))
@transform('demux*.fastq', regex(r'demux(.*).fastq'), r'STARmapped/\1.bam')
def STARmap(infile,outfile):
    ''' maps non-repetitive elements to genome '''
    outprefix = P.snip(outfile, ".bam")
    job_threads = PARAMS["STARmap_threads"]
    statement = '''
    STAR  --runMode alignReads
    --runThreadN %(job_threads)i
    --genomeDir %(STARmap_genome)s
    --readFilesIn %(infile)s
    --outSAMunmapped Within
    --outFilterMultimapNmax 10
    --limitBAMsortRAM 10000000000
    --outFilterMultimapScoreRange 1
    --outFileNamePrefix %(outprefix)s
    --outSAMattributes All
    --outStd BAM_SortedByCoordinate
    --outSAMtype BAM SortedByCoordinate
    --outFilterType BySJout
    --outReadsUnmapped Fastx
    --outFilterScoreMin 10
    --outSAMattrRGline ID:foo
    --alignEndsType Extend5pOfRead1
    > %(outfile)s 
    '''
    P.run()
#
    #NO LONGER NECESSARY
## series of commands to fix bam file names to call UMI
#@transform(STARmap, suffix('.bam'), '.new.bam')
#def umifix(infile, outfile):
#    statement = ''' samtools view -h -o %(infile)s.sam %(infile)s &&
#    cut -f1 %(infile)s.sam | sed 's/\(.*\):/\1_/' >%(infile)s.names.txt &&
#    cut -f 2- %(infile)s.sam > %(infile)s.nonames.txt &&
#    paste %(infile)s.names.txt %(infile)s.nonames.txt > %(infile)s.new.sam &&
#    samtools view -h -o %(outfile)s %(infile)s.new.sam '''
#    P.run()


##samtools sort
#@transform(STARmap, suffix('.bam'), '.sorted.bam')
#def samtools_sort(infile, outfile):
#    statement = ''' samtools sort %(infile)s -o %(outfile)s
#    '''
#    P.run()
    
    
# samtools index1
@transform(STARmap, suffix('.bam'), '.bam.bai')
def index1(infile, outfile):
    statement = ''' samtools index %(infile)s
    '''
    P.run()


# Deduplicate
@follows(index1)
@transform(STARmap, regex(r'(.*).bam'), r'\1.dedup.bam')
def dedup(infile,outfile):
    ''' deduplicate samples based on UMI using umi_tools '''
    statement = '''
    umi_tools dedup -I %(infile)s --output-stats=deduplicated -S %(outfile)s
    '''
    P.run()


# samtools index2
@transform(dedup, suffix('.dedup.bam'), 'dedup.bam.bai')
def index2(infile, outfile):
    ''' creates index deduplicated bam file, generates .bai '''
    statement = '''samtools index %(infile)s
    '''
    P.run()


## Make bigwig
#@follows(index2)
#@transform(dedup, suffix('.dedup.bam'), '.bw')
#def makeBigWig(infile, outfile):
#    ''' Makes bigwig files for visualization 
#    #### not working because I only have read 1!!!!! '''
#    statement = ''' 
#    PATH=/t1-data/user/nsampaio/py36-v1/conda-install/envs/Py2/bin;
#    CONDA_PREFIX=/t1-data/user/nsampaio/py36-v1/conda-install/envs/Py2;
#    /t1-data/user/nsampaio/software/gscripts/gscripts/general/make_bigwig_files.py
#    --bam %(infile)s
#    --genome %(general_chromsize)s
#    --bw_pos %(outfile)
#    '''
#    P.run()
#  
#    
## Call peaks
#@follows(index2)
#@transform(dedup, suffix('.dedup.bam'), '.bed')
#def callPeaks(infile, outfile):
#    ''' Calls peaks using CLIPper package'''
#    statement = '''
#    PATH=/t1-data/user/nsampaio/py36-v1/conda-install/envs/clipper/bin;
#    CONDA_PREFIX=/t1-data/user/nsampaio/py36-v1/conda-install/envs/clipper;
#    clipper -b %(infile)s -s hg19 -o %(outfile)s
#    '''
#    P.run() ###### need to get clipper to work and figure out commands
#
#
## Fix scores
#@transform(callPeaks, suffix('.bed'), '.fixed.bed')
#def fixScores(infile, outfile):
#    ''' Fixes p-values to be bed compatible '''
#    statement = ''' 
#    PATH=/t1-data/user/nsampaio/py36-v1/conda-install/envs/Py2/bin;
#    CONDA_PREFIX=/t1-data/user/nsampaio/py36-v1/conda-install/Py2/iCount;
#    python /t1-data/user/nsampaio/software/gscripts/gscripts/clipseq/fix_scores.py
#    --bed %(infile)s
#    --out_file %(outfile)s
#    '''
#    P.run()
#
#
## Bed to BigBed
#@transform(fixScores, suffix('.fixed.bed'), '.fixed.bb')
#def bigBed(infile, outfile):
#    ''' Converts bed file to bigBed file for uploading to the genomeBrowser '''
#    statement = ''' bedToBigBed %(infile)s
#    %(general_chromsize)s
#    %(outfile)s
#    -type=bed6+4
#    '''
#    P.run()
#
#
#    
#@follows(countRep, fastqc1, fastqc2, callPeaks, bigBed, makeBigWig)
#def full():
#    pass

@follows(STARmap, index1, dedup, index2)
def full():
    pass
    
# ---------------------------------------------------
# Generic pipeline tasks



def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)


if __name__ == "__main__":
    sys.exit(P.main(sys.argv))

